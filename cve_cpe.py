import requests
import json
import os
import csv
from time import sleep
# from bs4 import BeautifulSoup

'''
CPE模板 cpe:2.3:a/o/h:发布公司/主体:受影响实体/产品:版本:发布人:修改人:语言
a 应用
h 硬件
o 系统

此程序实现三个步骤：
1. url通过字符串拼接，将https://services.nvd.nist.gov/rest/json/cve/1.0/ 和 之前爬取到的cve编号拼接构成可访问的url，例如：
    https://services.nvd.nist.gov/rest/json/cve/1.0/cve-2021-41817
2. 网页cpe爬取，从访问url返回的json数据中提取cve编号、cpe信息、受影响起始版本、不受影响起始版本
3. 将数据以： cve-cpe-startVersion-endVersion逐条存入csv文件
    此处会有一个cve对应多个cpe情况出现，但我没想到更简便的存储方法，或许后期存入mongodb的时候会方便点？
'''

url='https://services.nvd.nist.gov/rest/json/cve/1.0/cve-2021-41817'
headers = {
      'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:87.0) Gecko/20100101 Firefox/87.0'
    }

# 1. url拼接,参数year指定年份文件夹,返回一个url_list
def url_splice(year):
    list_url=[]
    list_dir = os.listdir(year)  # 此处年份文件夹手动更改
    for list_txt in list_dir:
        f = open('./'+year+'/' + list_txt, 'r', encoding='utf-8')
        text_info = f.readlines()

        for content in text_info:
            content = content.rstrip('\n')
            split_list = content.split(',')
            cve_ID = split_list[-1]
            # print(cve_ID)
            url = 'https://services.nvd.nist.gov/rest/json/cve/1.0/'+cve_ID
            list_url.append(url)
        f.close()
    return list_url


# 2. cpe爬取,参数url_list来自url_splice函数，返回一个cve_list
def cve_cpe_crawler(url,headers):
    json_extract_list=[]
    # for url in url_list:
    page_text = requests.get(url=url, headers=headers)
    page_text.encoding = 'utf-8'
    # 将网页响应的json数据转换成dict
    # try:
    json_content=json.loads(page_text.text)
    # except:
    #     return json_extract_list
    # print(type(json_content))
    # 第一步：提取result信息，数据类型依然是dict
    try:
        json_result=json_content['result']
    except:
        return json_extract_list
    # print(type(json_result))
    # 第二步：提取CVE_Items,数据类型是list,但第一个值是dict类型
    json_CVE_Items=json_result['CVE_Items']
    # print(json_CVE_Items[0])
    # print(type(json_CVE_Items[0]))
    # 第三步：提取cve，数据类型是dict
    json_CVE_Items_dict=json_CVE_Items[0]
    json_cve=json_CVE_Items_dict['cve']
    # print(json_cve)
    # print(type(json_cve))
    # 第四步：提取cveID
    json_cveID=json_cve['CVE_data_meta']['ID']
    print(json_cveID)
    # 第五步：提取configuration，数据类型是dict
    json_configuration=json_CVE_Items_dict['configurations']
    # print(json_configuration)
    # print(type(json_configuration))
    # 第六步：提取nodes下的第0个元素，数据类型是dict
    try:
        json_nodes_0=json_configuration['nodes'][0]
    except:
        return json_extract_list
    # print(json_nodes_0)
    # print(type(json_nodes_0))
    # 第七步：提取cpe_match，数据类型是list
    json_cpe_match=json_nodes_0['cpe_match']
    # print(json_cpe_match)
    # print(type(json_cpe_match))
    # 第八步：提取所有元素中cpe23Uri、versionStartIncluding、versionEndExcluding
    for index in json_cpe_match:
        # 每一个index数据类型都是dict
        # print(index)
        # print(type(index))
        json_cpe23Uri=index['cpe23Uri']
        # list的index方法如果没有查找到指定key值会抛出异常，此处做异常处理
        try:
            json_versionStartIncluding=index['versionStartIncluding']
        except:
            json_versionStartIncluding='0.0.0'
        try:
            json_versionEndExcluding=index['versionEndExcluding']
        except:
            json_versionEndExcluding='0.0.0'
        # print(json_cpe23Uri)
        # print(json_versionStartIncluding)
        # print(json_versionEndExcluding)
        json_extract_list.append([json_cveID,json_cpe23Uri,json_versionStartIncluding,json_versionEndExcluding])
        sleep(0.5)
    # print(json_extract_list)
    # break
    return json_extract_list

# 3. 按年份存入csv,参数year来自year_list，cve_list来自cve_cpe_crawler函数
def save_to_csv(year,cve_list):
    # year_list=['1988','1989','1990','1991','1992','1993','1994','1995','1996','1997','1998','1999','2000','2001','2002','2003','2004','2005','2006','2007','2008','2009','2010','2011','2012','2013','2014','2015','2016','2017','2018','2019','2020','2021','2022']

    f_csv=open('./years'+'/'+year+'.csv','a',encoding='utf-8',newline='')
    csv_write=csv.writer(f_csv)
    # 写入标题头
    # head=['cveID','cpe23Uri','versionStartIncluding','versionEndExcluding']
    # csv_write.writerow(head)
    # 写入内容
    if cve_list:
        for cve_info in cve_list:
            csv_write.writerow(cve_info)
    f_csv.close()

if __name__=="__main__":
    year_list = ['1988', '1989', '1990', '1991', '1992', '1993', '1994', '1995', '1996', '1997', '1998', '1999', '2000',
                 '2001', '2002', '2003', '2004', '2005', '2006', '2007', '2008', '2009', '2010', '2011', '2012', '2013']
    #
    # for url in url_splice(year_list[-1]):
    #     print(url)
    #
        # for cve_info in cve_cpe_crawler(url,headers):
        #     print(cve_info)
    year=year_list[-1]
    url_list=url_splice(year)
    for url in url_list:
        save_to_csv(year,cve_cpe_crawler(url,headers))
    # cve_cpe_crawler(url_splice(year), headers)